<h1>Hi, I'm Aishwarya! <br/><a href="https://www.linkedin.com/in/aishwarya-chennabathni/">Data Scientist</a></h1>

<h2>ğŸ‘¨â€ğŸ’» Introduction:</h2>

Iâ€™m a Data Scientist with 6+ years of experience delivering end-to-end AI/ML and GenAI solutions across healthcare, logistics, insurance, real estate, and the public sector. I build production systemsâ€”from forecasting and pricing to RAG assistants and document intelligenceâ€”grounded in strong MLOps, measurable impact, and clear communication with business leaders.&#x20;

**ğŸ’¡ What I Do:**

* **GenAI & NLP**: RAG apps (LangChain, Llamaindex, FAISS), GPT-4â€“powered assistants, semantic search, summarization, NER, OCR + entity extraction.
* **Forecasting & optimization**: SARIMA/Prophet/LSTM for demand, inventory, and capacity; elasticity modeling and causal A/B testing for pricing.
* **Predictive ML**: XGBoost/LightGBM/Scikit-learn pipelines for churn, risk, and classification with SHAP/LIME for explainability.
* **Analytics to action**: translate insights into workflows and dashboards; lead Agile sprints, reviews, and stakeholder readouts.

**âš™ Tech Stack:**

* **Languages:** Python, SQL
* **ML/DL:** Pandas, NumPy, Scikit-learn, PyTorch, TensorFlow, XGBoost, LightGBM, Hugging Face, Statsmodels
* **Data & Orchestration:** Snowflake, Redshift, BigQuery, Spark/PySpark, dbt, Airflow
* **MLOps & Cloud:** MLflow, Docker, GitHub Actions CI/CD; AWS SageMaker, Azure ML, GCP Vertex AI
* **GenAI/RAG:** GPT-4, LangChain, FAISS, Prompt Engineering; governance with SHAP/LIME and compliance (FOIP/HIA/GDPR)

**ğŸ“ˆ Recent Highlights:**

* Shipped a secure GPT-4 virtual assistant for Ontarioâ€™s Ministry of Health (RAG over policy + Snowflake), cutting inquiry resolution time by 60% and improving accessibility.
* Built multi-seasonal LTC bed-demand forecasting (SARIMA) with drift diagnostics and MLflow tracking, embedded in operational dashboards.
* Delivered churn prediction and pricing optimization for logistics, improving early churn detection and informing revenue-positive pricing moves.

**ğŸ” Interests:**
Responsible AI, LLMOps (prompt versioning, drift monitoring), retrieval quality, and turning analytical models into reliable, user-centric products.

## ğŸ’¼ Work Experience
- ğŸ¢ **Ministry of Health Ontario, Ministry of Long-Term Care** Â· ğŸ“ **Toronto, ON** Â· ğŸ“… **Sept 2024 â€“ Present**
- ğŸ¢ **Tiger Analytics** Â· ğŸ“… **May 2022â€“ June 2024**
- ğŸ¢ **Optum Global Solutions, United Health Group** Â· ğŸ“… **July 2017â€“ August 2020**

## ğŸ“ Education
- ğŸ“ **Master of Business Administration in STEM - Data Science, AI/ML and Analytics** Â· ğŸ“… **May 2020â€“ May 2022**             
Indian Institute of Management Tiruchirappalli              
Focused on applied machine learning, statistics, and business analytics      

- ğŸ“ **Bachelor of Technology in Electrical and Electronics Engineering** Â· ğŸ“… **Sept 2013â€“July 2017**          
VNR Vignana Jyothi Institute of Engineering and Technology

<h2>ğŸ’¡ My Portfolio Projects </h2>

Welcome to my GitHub portfolio! Here are some of the projects I've worked on.

### ğŸ§  Advanced RAG Pipeline â€” LlamaIndex + TruLens

**Description:** Built and evaluated a production-minded RAG system that pairs **Sentence-Window Retrieval (SWR)** for high-precision local context with **Auto-Merging Retrieval (AMR)** for coherent multi-chunk context. Quality is measured via the **RAG-Triad**â€”**Context Relevance, Groundedness, Answer Relevance**â€”with a leaderboard tracking latency, tokens, and cost.

**Models / Tools / Techniques:** LlamaIndex Â· TruLens Â· OpenAI/embedding models Â· Sentence-Window Retrieval Â· Auto-Merging Retrieval Â· Hierarchical chunking Â· Cross-encoder reranking (BGE) Â· VectorStoreIndex (persisted) Â· Prompt budgeting
Evaluation: RAG-Triad (CR/G/AR) Â· Leaderboard (latency/tokens/cost) Â· A/B retrieval experiments (window size, hierarchy depth)

**Highlights:** SWR + AMR outperform dense baseline on **grounding** and **context relevance** Â· Coherent, non-fragmented context via AMR Â· Token/cost reduction vs naÃ¯ve top-k Â· Reproducible pipeline with persisted indexes and TruLens dashboards

[**Open Colab â€” Advanced\_RAG\_Pipeline**](https://github.com/Aishwarya-chen11/Implementing-Advanced-RAG-techniques/blob/main/Advanced_RAG_Pipeline.ipynb)

[**Open Colab â€” RAG\_Triad\_of\_metrics**](https://github.com/Aishwarya-chen11/Implementing-Advanced-RAG-techniques/blob/main/RAG_Triad_of_metrics.ipynb)

[**Open Colab â€” Sentence\_window\_retrieval**](https://github.com/Aishwarya-chen11/Implementing-Advanced-RAG-techniques/blob/main/Sentence_window_retrieval.ipynb)

[**Open Colab â€” Auto-merging\_Retrieval**](https://github.com/Aishwarya-chen11/Implementing-Advanced-RAG-techniques/blob/main/Auto-merging_Retrieval.ipynb)

[**GitHub Repo**](https://github.com/Aishwarya-chen11/Implementing-Advanced-RAG-techniques)

### ğŸ“© Spam SMS Classification & Data Analysis â€” GPT-2 Fine-Tuning

**Description:** Fine-tuned **GPT-2 (124M)** with a lightweight classifier head to detect **spam vs ham** on the **UCI SMS Spam Collection**. Built a **repeatable, auditable** workflow: deterministic preprocessing & splits, balanced training subset for pedagogy, and clear evaluation with accuracy/loss curves plus optional PR/ROC on the original imbalance.

**Models / Tools / Techniques:** GPT-2 small (decoder-only) Â· PyTorch Â· `tiktoken` (byte-level BPE) Â· AdamW Â· Selective unfreezing (last block + LN + Linear 768â†’2) Â· Last-token logits for CE loss Â· Pandas/NumPy Â· Matplotlib
Evaluation & Ops: Accuracy/Loss curves Â· (Optional) Precision/Recall/F1, ROC-AUC Â· Threshold calibration Â· Seeded splits Â· CPU/GPU friendly

**Highlights:** Fast convergence (â‰ˆ4â€“5 epochs) on balanced subset Â· Robust to obfuscation (emoji/unicode) thanks to byte-level BPE Â· Minimal compute, production-style head Â· Artifacts saved (`loss-plot.pdf`, `accuracy-plot.pdf`, `review_classifier.pth`) Â· Ready for drift monitoring & recalibration

[**Open Colab**](https://github.com/Aishwarya-chen11/Fine-tuned-LLM-Classification-Model/blob/main/Fine_tuned_LLM_classification_model.ipynb)  Â·  [**GitHub Repo**](https://github.com/Aishwarya-chen11/Fine-tuned-LLM-Classification-Model)

### ğŸ–¼ï¸ Multi-Modal RAG with LangChain (PDF + Images)

**Description:** Built a **multimodal RAG** pipeline that unifies **text and images** from PDFs in one vector space using **CLIP ViT-B/32**. Extracts page text & figures with **PyMuPDF**, embeds both into **512-d unit vectors**, retrieves cross-modal matches via **FAISS**, then formats a **multimodal prompt** (text + base64 images) for **GPT-4.1** to produce grounded answers citing the exact page and figure.

**Models / Tools / Techniques:** LangChain Â· PyMuPDF (fitz) Â· Pillow Â· CLIP ViT-B/32 (Transformers) Â· FAISS Â· GPT-4.1 (vision)
Unified embeddings (text+image) Â· L2-normalization (512-d) Â· RecursiveCharacterTextSplitter (500/100) Â· similarity\_search\_by\_vector Â· Multimodal prompt (image\_url + base64)

**Highlights:** One embedding model â†’ **cross-modal retrieval** Â· Answers grounded with **page snippets + figures** Â· No captioning required Â· Simple, fast stack (CLIP + FAISS) Â· Reproducible notebook with example queries (â€œWhat does the chart on page 1 show?â€)

[**Open Colab**](https://github.com/Aishwarya-chen11/Build-MultiModal-RAG-with-Langchain/blob/main/multimodalopenai.ipynb) Â· [**GitHub Repo**](https://github.com/Aishwarya-chen11/Build-MultiModal-RAG-with-Langchain)

### ğŸ“ Instruction Fine-Tuning a Decoder-Only LLM (GPT-2 Medium)

**Description:** End-to-end **SFT** pipeline that turns a pretrained **GPT-2 Medium (355M)** into a better instruction follower. Converts raw JSON to **Alpaca-style** prompts, applies **dynamic padding** with **label masking** (train on **response tokens only**), fine-tunes with **AdamW**, runs deterministic inference, and performs lightweight **LLM-as-a-judge** scoring for quick quality checks.

**Models / Tools / Techniques:** PyTorch Â· GPT-2 Medium (355M) Â· `tiktoken` (GPT-2 BPE) Â· Dynamic padding Â· `ignore_index=-100` label masking Â· Cross-entropy over response tokens Â· AdamW (5e-5) Â· Deterministic generation Â· Optional **Ollama Llama-3** judge Â· `tqdm`

**Highlights:** Visible instruction adherence after **1 epoch** Â· Reproducible seeds & saved artifacts (JSON preds + checkpoint) Â· Clean, didactic code (no LoRA; easy to extend) Â· CPU/Single-GPU friendly Â· Simple to swap schedulers/PEFT later

[**Open Colab**](https://github.com/Aishwarya-chen11/LLM-Instruction-Fine-tuning/blob/main/Instruction_Fine_Tuning_LLM.ipynb) Â· [**GitHub Repo**](https://github.com/Aishwarya-chen11/LLM-Instruction-Fine-tuning)

### ğŸ§© Building LLMs From Scratch â€” GPT-Style (PyTorch)

**Description:** Implemented a GPT-style decoder-only Transformer **from first principles**: custom tokenizer â†’ input/target maker â†’ dataloader â†’ token + positional embeddings â†’ **causal multi-head self-attention** (fused QKV) â†’ **GELU** MLP â†’ **pre-norm + residuals** â†’ decoding (**temperature / top-k**) â†’ loss & **perplexity**. Validated wiring by mapping **GPT-2 (124M)** weights.

**Models / Tools / Techniques:** PyTorch Â· NumPy Â· (optional) `tiktoken` / `transformers` Â· Causal mask Â· Multi-head attention (fused QKV) Â· GELU MLP Â· LayerNorm (pre-norm) Â· Tied embeddings Â· Top-k sampling & temperature Â· Perplexity tracking

**Highlights:** End-to-end working GPT in a single annotated notebook Â· Correct causal masking & pre-norm stack Â· GPT-2 weight mapping sanity-checks the implementation Â· Colab-friendly; lightweight text demo (`the-verdict.txt`) for quick runs

[**Open Colab**](https://github.com/Aishwarya-chen11/Build-LLM-architecture-from-scratch/blob/main/Building_LLM_from_Scratch.ipynb) Â· [**GitHub Repo**](https://github.com/Aishwarya-chen11/Build-LLM-architecture-from-scratch)

<h2> ğŸ¤³ Connect with me:</h2>

Feel free to reach out if you have any questions or would like to collaborate on a project!

[<img align="left" alt="JoshMadakor | LinkedIn" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/linkedin.svg" />][linkedin]
[<img align="left" alt="Aishwarya Chennabathni | Gmail" width="22px" src="https://cdn.jsdelivr.net/npm/simple-icons@v3/icons/gmail.svg" />][gmail]

[linkedin]:https://www.linkedin.com/in/aishwarya-chennabathni/
[gmail]:mailto:aishwarya.chen11@gmail.com


